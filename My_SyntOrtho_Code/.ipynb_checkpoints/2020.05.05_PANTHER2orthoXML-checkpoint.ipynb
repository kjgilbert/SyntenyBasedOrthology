{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import ete3\n",
    "from tqdm import tqdm\n",
    "from ete3 import orthoxml\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_PANTHER_families(tree_files, output_file, orthoxml_name, pthfam_filter=set(), origin='PANTHER v.14.1', version='0.3', originVersion='0.2'):\n",
    "    '''\n",
    "    Convert PANTHER family trees to one orthoXML file and a mapper between new HOG ids and PANTHER ancestral node ids \n",
    "    '''        \n",
    "    # add an ortho group container to the orthoXML document\n",
    "    ortho_groups = orthoxml.groups()\n",
    "    xml = orthoxml.orthoXML(origin=origin, version=version, originVersion=originVersion)\n",
    "    xml.set_groups(ortho_groups)\n",
    "\n",
    "    # mapping between new HOG ids and panther fam:node ids\n",
    "    fam_hog2pthfam_ans_outf = open('{}{}_fam_hog_pthfam_ans.txt'.format(output_path, orthoxml_name), 'w')\n",
    "    gene2leafhog_outf = open('{}{}_gene2leafhog.txt'.format(output_path, orthoxml_name), 'w')\n",
    "    \n",
    "    # to later retrieve sequences in alignment files\n",
    "    pthfam_an2prot_outf = open('{}{}_pthfam_an2prot.txt'.format(output_path, orthoxml_name), 'w')\n",
    "            \n",
    "    # initiate custom orthoxml identifiers\n",
    "    fam_id = 1\n",
    "    unique_id = 1\n",
    "\n",
    "    # store orthoxml species and gene objects\n",
    "    sp2genes = {}\n",
    "    \n",
    "    for tf in tqdm(tree_files):\n",
    "#     for tf in tree_files:\n",
    "        \n",
    "        pthfam_id = tf.split('/')[-1].rstrip('.tree')\n",
    "        \n",
    "        # way of ignoring some families\n",
    "        if pthfam_id in pthfam_filter:\n",
    "            continue\n",
    "\n",
    "        with open(tf) as inf: # opens file into variable inf\n",
    "            tree_str = inf.readline() # read (?)first line into the tree string\n",
    "            # mapper to speciesname (mnemonic), source database, uniprot id \n",
    "            leaf_id2info = {}\n",
    "            for l in inf:\n",
    "                x = l.rstrip().split('|')\n",
    "                y = x[0].split(':')\n",
    "                leaf_id2info[y[0]]=(y[1], x[1], x[2].split('=')[1][:-1])\n",
    "        \n",
    "        #########################################################################################################\n",
    "        # Keep track of custom orthoxml gene ids\n",
    "        leaf_id2gene_id = {}\n",
    "        pthfam_an_id2prot_id = {}\n",
    "        for leaf_id, (spname, dbname, uniprot_id) in leaf_id2info.items(): # loop through the dictionary, the dictionary will have the left name and the 3 items within it\n",
    "\n",
    "            # create species and genes containers # other containers in the output file\n",
    "            if spname not in sp2genes:\n",
    "                sp = orthoxml.species(spname)\n",
    "                db = orthoxml.database(name=dbname) # populating the containers ( or creating them)\n",
    "                sp.add_database(db) # making db a sub-container of species\n",
    "                genes = orthoxml.genes()\n",
    "                db.set_genes(genes)\n",
    "                sp2genes[spname] = genes\n",
    "                # add info to the orthoXML document\n",
    "                xml.add_species(sp)\n",
    "            else:\n",
    "                genes = sp2genes[spname]\n",
    "\n",
    "            # store the leaf gene in 'genes' of 'sp'\n",
    "            gn = orthoxml.gene(protId=uniprot_id, id=unique_id)\n",
    "            genes.add_gene(gn)\n",
    "\n",
    "            # track gene id\n",
    "            leaf_id2gene_id[leaf_id]=unique_id\n",
    "            unique_id += 1\n",
    "            \n",
    "            pthfam_an_id2prot_id[':'.join([pthfam_id, leaf_id])]=uniprot_id\n",
    "            \n",
    "        # another mapper \n",
    "        for pthfam_an_id, prot_id in pthfam_an_id2prot_id.items():\n",
    "            pthfam_an2prot_outf.write('{}\\t{}\\n'.format(pthfam_an_id, prot_id))\n",
    "\n",
    "        tree = ete3.Tree(tree_str)\n",
    "        \n",
    "        #########################################################################################################\n",
    "        # Split the PANTHER family if starts by duplication events and at each HGT event\n",
    "        split_families = []\n",
    "\n",
    "        # OrthoXML does not support duplication events to be at the root\n",
    "        # of the tree, so we search for the top most speciation events in\n",
    "        # the tree and export them as separate ortholog groups\n",
    "        is_speciation = lambda n: getattr(n, 'Ev', \"\") == \"0>1\" or not n.children\n",
    "        dupl_families = list(tree.iter_leaves(is_leaf_fn=is_speciation))\n",
    "        \n",
    "        for dfam in dupl_families:\n",
    "            # track leaves remaining after HGT splits\n",
    "            remaining_leaves = set(dfam.get_leaf_names())\n",
    "            parent2speroots = defaultdict(list)\n",
    "\n",
    "            # bottom-up traversal\n",
    "            for node in dfam.traverse(\"postorder\"):\n",
    "\n",
    "                # leaf or speciation --> create a new speciation root (potential start for a new family)\n",
    "                if node.is_leaf() or node.Ev == '0>1':\n",
    "                    parent2speroots[node.up].append(node)\n",
    "\n",
    "                # duplication --> extend with the same speciation roots\n",
    "                elif node.Ev == '1>0':\n",
    "                    parent2speroots[node.up].extend(parent2speroots[node])\n",
    "\n",
    "                # HGT --> store its speciation roots as new families and update remaining leaves \n",
    "                elif node.Ev == '0>0':\n",
    "                    for speroot in parent2speroots[node]:\n",
    "\n",
    "                        # get remaining leaves of speciation root\n",
    "                        speroot_leaves = speroot.get_leaf_names()\n",
    "                        rem_speroot_leaves = list(remaining_leaves.intersection(speroot_leaves))\n",
    "\n",
    "                        # cases where all leaves have already been pruned (e.g. both children are HGT events)\n",
    "                        if rem_speroot_leaves:\n",
    "                            # copy the subtree\n",
    "                            new_tree = speroot.copy()\n",
    "                            # prune it\n",
    "                            new_tree.prune(rem_speroot_leaves)\n",
    "                            split_families.append(new_tree)\n",
    "                            # update remaining leaves\n",
    "                            remaining_leaves = remaining_leaves.difference(speroot_leaves)\n",
    "\n",
    "            # finish with root!\n",
    "            if remaining_leaves:\n",
    "                dfam.prune(list(remaining_leaves))\n",
    "                split_families.append(dfam)\n",
    "\n",
    "        #########################################################################################################\n",
    "        # Convert split families to orthoXML\n",
    "        for fam in split_families:\n",
    "            \n",
    "            # to bookeep mapping to functional information\n",
    "            hog_id2an_ids = defaultdict(list)\n",
    "            \n",
    "            # because leaf HOGs are implicit in the orthoXML format\n",
    "            gene_id2hog_id = {}\n",
    "            \n",
    "            if fam.is_leaf():\n",
    "                continue\n",
    "\n",
    "            assert fam.Ev=='0>1', 'A family should start with a speciation node'\n",
    "\n",
    "            node2group = {}\n",
    "\n",
    "            # create the root-HOG\n",
    "            taxon = orthoxml.property('TaxRange', fam.S)\n",
    "            node2group[fam] = orthoxml.group(id=fam_id, property=[taxon])\n",
    "            ortho_groups.add_orthologGroup(node2group[fam])\n",
    "\n",
    "            # keep track of the current subhog id\n",
    "            hog_id2curr_subhog_id = {fam_id:1}\n",
    "\n",
    "            # top-down traversal\n",
    "            for node in fam.traverse(\"preorder\"):\n",
    "                if node.is_leaf():\n",
    "                    continue\n",
    "\n",
    "                group = node2group[node]\n",
    "                hog_id = group.id\n",
    "                event = getattr(node, \"Ev\")\n",
    "\n",
    "                for child in node.children:\n",
    "                    if child.is_leaf():\n",
    "                        \n",
    "                        # Add gene to the group \n",
    "                        gene_id = leaf_id2gene_id[child.name]\n",
    "                        group.add_geneRef(orthoxml.geneRef(id=gene_id))\n",
    "                        \n",
    "                        # A duplication here means a leaf (implicit) HOG\n",
    "                        if event == \"1>0\":\n",
    "                            child_hog_id = \"{}.{}\".format(hog_id, hog_id2curr_subhog_id[hog_id])\n",
    "                            hog_id2curr_subhog_id[hog_id] += 1\n",
    "                            hog_id2an_ids[child_hog_id].append(child.name)\n",
    "                            \n",
    "                            # store a mapping to its HOG id\n",
    "                            gene_id2hog_id[gene_id] = child_hog_id\n",
    "                        else:\n",
    "                            hog_id2an_ids[hog_id].append(child.name)\n",
    "                        \n",
    "                        continue\n",
    "\n",
    "                    child_event = getattr(child, \"Ev\")\n",
    "\n",
    "                    # A duplication here means no new HOG\n",
    "                    if child_event == \"1>0\":\n",
    "                        node2group[child] = orthoxml.group(id=hog_id)\n",
    "                        group.add_paralogGroup(node2group[child])\n",
    "                        hog_id2an_ids[hog_id].append(child.ID)\n",
    "\n",
    "                    else:\n",
    "                        taxon = orthoxml.property('TaxRange', child.S)\n",
    "\n",
    "                        # A speciation following a duplication means a new (explicit) HOG\n",
    "                        if event == \"1>0\": \n",
    "                            child_hog_id = \"{}.{}\".format(hog_id, hog_id2curr_subhog_id[hog_id])\n",
    "                            hog_id2curr_subhog_id[hog_id] += 1\n",
    "                            hog_id2curr_subhog_id[child_hog_id] = 1\n",
    "                            node2group[child] = orthoxml.group(id=child_hog_id, property=[taxon])\n",
    "                            hog_id2an_ids[child_hog_id].append(child.ID)\n",
    "                        else:\n",
    "                            node2group[child] = orthoxml.group(id=hog_id, property=[taxon])\n",
    "                            hog_id2an_ids[hog_id].append(child.ID)\n",
    "\n",
    "                        group.add_orthologGroup(node2group[child])\n",
    "\n",
    "            # store mapping between new HOG ids and panther fam:node ids\n",
    "            for hog_id, an_ids in hog_id2an_ids.items():\n",
    "                fam_hog2pthfam_ans_outf.write('{}\\t{}\\t{}\\t{}\\n'.format(fam_id, hog_id, pthfam_id, ';'.join(an_ids)))\n",
    "            \n",
    "            # write mapper gene_id_2hog_id (only implicit HOGs)\n",
    "            for gene_id, hog_id in gene_id2hog_id.items():\n",
    "                gene2leafhog_outf.write('{}\\t{}\\n'.format(gene_id, hog_id))\n",
    "        \n",
    "            fam_id += 1\n",
    "\n",
    "    fam_hog2pthfam_ans_outf.close()\n",
    "    gene2leafhog_outf.close()\n",
    "    pthfam_an2prot_outf.close()\n",
    "    \n",
    "    with open('{}{}_tmp.orthoxml'.format(output_path, orthoxml_name), 'w') as outf:\n",
    "        xml.export(outf, 0, namespace_=\"\")\n",
    "        \n",
    "    with open('{}{}_tmp.orthoxml'.format(output_path, orthoxml_name), 'r') as inf:\n",
    "        # skip first line\n",
    "        inf.readline()\n",
    "        xml_read = inf.read()\n",
    "\n",
    "    pat = re.compile(r'(.*=)b\\'(?P<byte>\\\".*\\\")\\'(.*)', re.MULTILINE)\n",
    "    xml_tmp = pat.sub(r'\\1\\2\\3', xml_read) # because two pattern per line\n",
    "    xml_write = pat.sub(r'\\1\\2\\3', xml_tmp)  \n",
    "\n",
    "    with open('{}{}.orthoxml'.format(output_path, orthoxml_name), 'w') as outf:\n",
    "        outf.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')  # not sure if necessary\n",
    "        outf.write('<orthoXML xmlns=\"http://orthoXML.org/2011/\" version=\"{}\" origin=\"{}\" originVersion=\"{}\">\\n'.format(version, origin, originVersion))\n",
    "        outf.write(xml_write)\n",
    "        \n",
    "    os.remove('{}{}_tmp.orthoxml'.format(output_path, orthoxml_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/Victor/Documents/UNIL/PhD/03_projects/04_functional_consequences/data/'\n",
    "sample_path = '{}sample/'.format(data_path)\n",
    "sample_family_path = '{}Tree_MSF/'.format(sample_path)\n",
    "\n",
    "\n",
    "tree_files = glob.glob(\"{}*.tree\".format(sample_family_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_PANTHER_families() missing 1 required positional argument: 'orthoxml_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-2bf100b618a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparse_PANTHER_families\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PANTHER v.14.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginVersion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: parse_PANTHER_families() missing 1 required positional argument: 'orthoxml_name'"
     ]
    }
   ],
   "source": [
    "parse_PANTHER_families(tree_files, sample_path, origin='PANTHER v.14.1', version='0.3', originVersion='0.2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small PANTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/Victor/Documents/UNIL/PhD/03_projects/04_functional_consequences/data/'\n",
    "tree_path = '{}PANTHER14.1_data/Tree_MSF/'.format(data_path)\n",
    "\n",
    "output_path = '{}orthoXML_small/'.format(data_path)\n",
    "orthoxml_name = 'panther'\n",
    "\n",
    "tree_files = glob.glob(\"{}*.tree\".format(tree_path))\n",
    "\n",
    "parse_PANTHER_families(tree_files[:10], output_path, orthoxml_name, origin='PANTHER v.14.1', version='0.3', originVersion='0.2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whole PANTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [58:47<00:00,  4.40it/s]   \n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/Victor/Documents/UNIL/PhD/03_projects/04_functional_consequences/data/'\n",
    "tree_path = '{}PANTHER14.1_data/Tree_MSF/'.format(data_path)\n",
    "\n",
    "output_path = '{}orthoXML/'.format(data_path)\n",
    "orthoxml_name = 'panther'\n",
    "\n",
    "tree_files = glob.glob(\"{}*.tree\".format(tree_path))\n",
    "\n",
    "parse_PANTHER_families(tree_files, output_path, orthoxml_name, origin='PANTHER v.14.1', version='0.3', originVersion='0.2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PTHR22911'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['PTHR22911'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [56:12<00:00,  4.60it/s]    \n"
     ]
    }
   ],
   "source": [
    "# without PTHR22911\n",
    "output_path = '{}orthoXML/'.format(data_path)\n",
    "orthoxml_name = 'panther_wo_PTHR22911'\n",
    "\n",
    "parse_PANTHER_families(tree_files, output_path, orthoxml_name, pthfam_filter=set(['PTHR22911']), origin='PANTHER v.14.1', version='0.3', originVersion='0.2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15524/15524 [1:02:24<00:00,  4.15it/s] \n"
     ]
    }
   ],
   "source": [
    "output_path = '{}orthoXML/by_pthr_fam/'.format(data_path)\n",
    "\n",
    "# one orthoXML per PANTHER family to debug PyHAM\n",
    "for tf in tqdm(tree_files):\n",
    "    pthfam_id = tf.split('/')[-1].rstrip('.tree')\n",
    "    parse_PANTHER_families([tf], output_path, pthfam_id, origin='PANTHER v.14.1', version='0.3', originVersion='0.2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/Victor/Documents/UNIL/PhD/03_projects/04_functional_consequences/data/'\n",
    "tree_path = '{}PANTHER14.1_data/Tree_MSF/'.format(data_path)\n",
    "output_path = '{}orthoXML_small/'.format(data_path)\n",
    "orthoxml_name = 'panther_fam2'\n",
    "tree_files = glob.glob(\"{}*.tree\".format(tree_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin='PANTHER v.14.1'\n",
    "version='0.3'\n",
    "originVersion='0.2'\n",
    "\n",
    "# Add an ortho group container to the orthoXML document\n",
    "ortho_groups = orthoxml.groups()\n",
    "xml = orthoxml.orthoXML(origin=origin, version=version, originVersion=originVersion)\n",
    "xml.set_groups(ortho_groups)\n",
    "\n",
    "# mapping between new HOG ids and panther fam:node ids\n",
    "fam_hog2pthfam_ans_outf = open('{}{}_fam_hog_pthfam_ans.txt'.format(output_path, orthoxml_name), 'w')\n",
    "gene2leafhog_outf = open('{}{}_gene2leafhog.txt'.format(output_path, orthoxml_name), 'w')\n",
    "\n",
    "# initiate custom orthoxml identifiers\n",
    "fam_id = 1\n",
    "unique_id = 1\n",
    "\n",
    "# store orthoxml species and gene objects\n",
    "sp2genes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTHR34460\n"
     ]
    }
   ],
   "source": [
    "tf = tree_files[1]\n",
    "pthfam_id = tf.split('/')[-1].rstrip('.tree')\n",
    "print(pthfam_id)\n",
    "\n",
    "with open(tf) as inf:\n",
    "    tree_str = inf.readline()\n",
    "    # mapper to speciesname (mnemonic), source database, uniprot id \n",
    "    leaf_id2info = {}\n",
    "    for l in inf:\n",
    "        x = l.rstrip().split('|')\n",
    "        y = x[0].split(':')\n",
    "        leaf_id2info[y[0]]=(y[1], x[1], x[2].split('=')[1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_id2gene_id = {}\n",
    "for leaf_id, (spname, dbname, uniprot_id) in leaf_id2info.items():\n",
    "\n",
    "    # create species and genes containers\n",
    "    if spname not in sp2genes:\n",
    "        sp = orthoxml.species(spname)\n",
    "        db = orthoxml.database(name=dbname)\n",
    "        sp.add_database(db)\n",
    "        genes = orthoxml.genes()\n",
    "        db.set_genes(genes)\n",
    "        sp2genes[spname] = genes\n",
    "        # add info to the orthoXML document\n",
    "        xml.add_species(sp)\n",
    "    else:\n",
    "        genes = sp2genes[spname]\n",
    "\n",
    "    # store the leaf gene in 'genes' of 'sp'\n",
    "    gn = orthoxml.gene(protId=uniprot_id, id=unique_id)\n",
    "    genes.add_gene(gn)\n",
    "\n",
    "    # track gene id\n",
    "    leaf_id2gene_id[leaf_id]=unique_id\n",
    "    unique_id += 1\n",
    "\n",
    "tree = ete3.Tree(tree_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "# Split the PANTHER family if starts by duplication events and at each HGT event\n",
    "split_families = []\n",
    "\n",
    "# OrthoXML does not support duplication events to be at the root\n",
    "# of the tree, so we search for the top most speciation events in\n",
    "# the tree and export them as separate ortholog groups\n",
    "is_speciation = lambda n: getattr(n, 'Ev', \"\") == \"0>1\" or not n.children\n",
    "dupl_families = list(tree.iter_leaves(is_leaf_fn=is_speciation))\n",
    "\n",
    "for dfam in dupl_families:\n",
    "    # track leaves remaining after HGT splits\n",
    "    remaining_leaves = set(dfam.get_leaf_names())\n",
    "    parent2speroots = defaultdict(list)\n",
    "\n",
    "    # bottom-up traversal\n",
    "    for node in dfam.traverse(\"postorder\"):\n",
    "\n",
    "        # leaf or speciation --> create a new speciation root (potential start for a new family)\n",
    "        if node.is_leaf() or node.Ev == '0>1':\n",
    "            parent2speroots[node.up].append(node)\n",
    "\n",
    "        # duplication --> extend with the same speciation roots\n",
    "        elif node.Ev == '1>0':\n",
    "            parent2speroots[node.up].extend(parent2speroots[node])\n",
    "\n",
    "        # HGT --> store its speciation roots as new families and update remaining leaves \n",
    "        elif node.Ev == '0>0':\n",
    "            for speroot in parent2speroots[node]:\n",
    "\n",
    "                # get remaining leaves of speciation root\n",
    "                speroot_leaves = speroot.get_leaf_names()\n",
    "                rem_speroot_leaves = list(remaining_leaves.intersection(speroot_leaves))\n",
    "\n",
    "                # cases where all leaves have already been pruned (e.g. both children are HGT events)\n",
    "                if rem_speroot_leaves:\n",
    "                    # copy the subtree\n",
    "                    new_tree = speroot.copy()\n",
    "                    # prune it\n",
    "                    new_tree.prune(rem_speroot_leaves)\n",
    "                    split_families.append(new_tree)\n",
    "                    # update remaining leaves\n",
    "                    remaining_leaves = remaining_leaves.difference(speroot_leaves)\n",
    "\n",
    "    # finish with root!\n",
    "    if remaining_leaves:\n",
    "        dfam.prune(list(remaining_leaves))\n",
    "        split_families.append(dfam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = split_families[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Magnoliophyta'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fam.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree node 'AN1' (0x120164e1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fam.children[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AMBTC', 'EnsemblGenome=AMTR_s00016p00259280', 'W1PFN3')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_id2info['AN1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bookeep mapping to functional information\n",
    "hog_id2an_ids = defaultdict(list)\n",
    "\n",
    "# because leaf HOGs are implicit in the orthoXML format\n",
    "gene_id2hog_id = {}\n",
    "\n",
    "if fam.is_leaf():\n",
    "    pass\n",
    "\n",
    "assert fam.Ev=='0>1', 'A family should start with a speciation node'\n",
    "\n",
    "node2group = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxon = orthoxml.property('TaxRange', fam.S)\n",
    "node2group[fam] = orthoxml.group(id=fam_id, property=[taxon])\n",
    "ortho_groups.add_orthologGroup(node2group[fam])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
